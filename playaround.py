# from typing import Dict, Optional, Tuple, List

# import flwr as fl
# import tensorflow as tf
# import numpy as np
# from keras.models import Sequential
# from keras import datasets, layers, models
# from keras.utils import np_utils
# from cinic10_ds import get_train_ds, get_test_val_ds
# import tensorflow_datasets as tfds
# import random
# import os
# from model import create_model
# from model_ascent import create_model_ascent
# import pandas as pd
import matplotlib.pyplot as plt
import absl.logging
absl.logging.set_verbosity(absl.logging.ERROR)
# from ray.util.multiprocessing import Pool

# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}
# np.set_printoptions(threshold=np.inf)

################# PLOTS HERE ####################

# title = "label 4 and 5 deleted in 70% of clients"
# fedval = [0.18043608963489532, 0.3378675878047943, 0.3620724081993103, 0.3992798626422882, 0.4974994957447052, 0.5699139833450317, 0.5951189994812012, 0.565913200378418, 0.6399279832839966, 0.6863372921943665, 0.695539116859436, 0.6859371662139893, 0.6887377500534058, 0.7201440334320068, 0.6871374249458313, 0.7267453670501709, 0.7351470589637756, 0.7479496002197266, 0.7405481338500977, 0.7413482666015625, 0.7347469329833984, 0.7619524002075195, 0.7665532827377319, 0.7375475168228149, 0.752550482749939, 0.728945791721344, 0.7725545167922974, 0.7609521746635437, 0.7769553661346436, 0.7667533755302429, 0.7757551670074463, 0.7793558835983276, 0.7849569916725159, 0.7847569584846497, 0.7769553661346436, 0.7807561755180359, 0.789557933807373, 0.7823565006256104, 0.7929586172103882, 0.7857571244239807, 0.774354875087738, 0.7811562418937683, 0.7849569916725159, 0.7845569252967834, 0.7757551670074463, 0.7881576418876648, 0.7941588163375854, 0.7903580665588379, 0.7973594665527344, 0.7837567329406738, 0.7915583252906799, 0.7905580997467041, 0.7899580001831055, 0.7029405832290649, 0.7949590086936951, 0.7891578078269958, 0.7909581661224365, 0.8013602495193481, 0.8003600835800171, 0.7911582589149475]
# fedavg = [0.2688537836074829, 0.38827764987945557, 0.41228246688842773, 0.4332866668701172, 0.5017003417015076, 0.5571114420890808, 0.5523104667663574, 0.5911182165145874, 0.6007201671600342, 0.622124433517456, 0.6213242411613464, 0.6459291577339172, 0.6309261918067932, 0.6473294496536255, 0.6483296751976013, 0.670734167098999, 0.6613322496414185, 0.6515303254127502, 0.6593318581581116, 0.6627325415611267, 0.6773354411125183, 0.6643328666687012, 0.6633326411247253, 0.6735346913337708, 0.6663332581520081, 0.670734167098999, 0.6699339747428894, 0.6689338088035583, 0.6657331585884094, 0.6681336164474487, 0.6865373253822327, 0.6899380087852478, 0.6763352751731873, 0.6755350828170776, 0.6787357330322266, 0.673734724521637, 0.690138041973114, 0.6773354411125183, 0.6817363500595093, 0.6807361245155334, 0.6905381083488464, 0.6947389245033264, 0.6897379755973816, 0.7257451415061951, 0.6809361577033997, 0.6817363500595093, 0.6957391500473022, 0.6829366087913513, 0.6825364828109741, 0.6865373253822327, 0.6833366751670837, 0.6851370334625244, 0.6829366087913513, 0.684736967086792, 0.6937387585639954, 0.6911382079124451, 0.6905381083488464, 0.6929385662078857, 0.693138599395752, 0.6893378496170044]
# fedprox1 = [0.24064813554286957, 0.39067813754081726, 0.4230846166610718, 0.4844968914985657, 0.5187037587165833, 0.5451090335845947, 0.5623124837875366, 0.5745149254798889, 0.596119225025177, 0.6105220913887024, 0.6249249577522278, 0.6245248913764954, 0.6465293169021606, 0.645729124546051, 0.6541308164596558, 0.6559311747550964, 0.6671334505081177, 0.664132833480835, 0.6639328002929688, 0.6623324751853943, 0.6599319577217102, 0.665333092212677, 0.674934983253479, 0.6751350164413452, 0.676135241985321, 0.7119423747062683, 0.6775355339050293, 0.6755350828170776, 0.6917383670806885, 0.6875374913215637, 0.6725345253944397, 0.6795359253883362, 0.6751350164413452, 0.6805360913276672, 0.6797359585762024, 0.6807361245155334, 0.6817363500595093, 0.6833366751670837, 0.6871374249458313, 0.6917383670806885, 0.6879376173019409, 0.6833366751670837, 0.6871374249458313, 0.7055411338806152, 0.6817363500595093, 0.695539116859436, 0.6937387585639954, 0.7189437747001648, 0.6917383670806885, 0.6927385330200195, 0.7069413661956787, 0.6989398002624512, 0.6961392164230347, 0.6911382079124451, 0.695539116859436, 0.6925384998321533, 0.6947389245033264, 0.6919384002685547, 0.7221444249153137, 0.6961392164230347]

title = "Dirichlet b=0.1"
fedval = [0.10022004693746567, 0.09761952608823776, 0.10262052714824677, 0.19863972067832947, 0.09981996566057205, 0.16183236241340637, 0.10322064161300659, 0.24584917724132538, 0.1796359270811081, 0.17803560197353363, 0.21344268321990967, 0.1970394104719162, 0.22824564576148987, 0.25505101680755615, 0.23464693129062653, 0.25205039978027344, 0.21584317088127136, 0.11122224479913712, 0.1554310917854309, 0.2020404040813446, 0.3722744584083557, 0.20484097301959991, 0.4340868294239044, 0.20384076237678528, 0.45689138770103455, 0.30626124143600464, 0.3820764124393463, 0.3722744584083557, 0.32166433334350586, 0.3524704873561859, 0.5217043161392212, 0.35887178778648376, 0.42008402943611145, 0.24684937298297882, 0.3350670039653778, 0.2936587333679199, 0.26265251636505127, 0.3792758584022522, 0.4058811664581299, 0.43728744983673096, 0.47149428725242615, 0.6315262913703918, 0.45689138770103455, 0.3916783332824707, 0.2836567461490631, 0.4404881000518799, 0.22364473342895508, 0.3274655044078827, 0.4502900540828705, 0.5497099161148071, 0.5311062335968018, 0.5993198752403259, 0.5787157416343689, 0.5501100420951843, 0.6097219586372375, 0.39287856221199036, 0.5819163918495178, 0.6119223833084106, 0.5093018412590027, 0.4332866668701172]
fedavg = [0.10182036459445953, 0.10182036459445953, 0.10562112182378769, 0.15463092923164368, 0.17903581261634827, 0.1278255581855774, 0.14982996881008148, 0.18883776664733887, 0.2018403708934784, 0.2142428457736969, 0.22224444150924683, 0.19823965430259705, 0.19843968749046326, 0.20604120194911957, 0.36287257075309753, 0.09761952608823776, 0.28405681252479553, 0.21844369173049927, 0.17383477091789246, 0.27425485849380493, 0.11302260309457779, 0.2164432853460312, 0.2876575291156769, 0.1894378811120987, 0.13002599775791168, 0.25965192914009094, 0.3942788541316986, 0.29445889592170715, 0.1254250854253769, 0.22484496235847473, 0.24984997510910034, 0.33906781673431396, 0.31406280398368835, 0.29425886273384094, 0.369273841381073, 0.26725345849990845, 0.2580516040325165, 0.3550710082054138, 0.36107221245765686, 0.10182036459445953, 0.30466094613075256, 0.330466091632843, 0.37267452478408813, 0.12602519989013672, 0.3748749792575836, 0.35047009587287903, 0.49909982085227966, 0.39747950434684753, 0.2514503002166748, 0.30166032910346985, 0.11882376670837402, 0.1946389228105545, 0.4382876455783844, 0.3346669375896454, 0.14842969179153442, 0.4430886209011078, 0.4476895332336426, 0.43268653750419617, 0.37327465415000916, 0.34386876225471497]
fedprox0001 = [0.19103820621967316, 0.14962992072105408, 0.10182036459445953, 0.1010202020406723, 0.09901980310678482, 0.09901980310678482, 0.21084216237068176, 0.19003801047801971, 0.1010202020406723, 0.10182036459445953, 0.1576315313577652, 0.10282056778669357, 0.18563713133335114, 0.11002200096845627, 0.12942588329315186, 0.19263853132724762, 0.2412482500076294, 0.11862372606992722, 0.17063412070274353, 0.1996399313211441, 0.2884576916694641, 0.18023604154586792, 0.20044009387493134, 0.15843167901039124, 0.2506501376628876, 0.24824965000152588, 0.10202040523290634, 0.15123024582862854, 0.3200640082359314, 0.25745150446891785, 0.20264053344726562, 0.38827764987945557, 0.2288457751274109, 0.3174634873867035, 0.2864573001861572, 0.16123224794864655, 0.25285056233406067, 0.398479700088501, 0.32406482100486755, 0.30646130442619324, 0.24104821681976318, 0.4660932123661041, 0.2486497312784195, 0.3122624456882477, 0.14382876455783844, 0.1156231239438057, 0.24004800617694855, 0.26425284147262573, 0.26245248317718506, 0.39487898349761963, 0.19303861260414124, 0.325065016746521, 0.1996399313211441, 0.2858571708202362, 0.2812562584877014, 0.47709542512893677, 0.28625723719596863, 0.3548709750175476, 0.4968993663787842, 0.33866772055625916]
fedprox1 = [0.10802160203456879, 0.10242048650979996, 0.10142028331756592, 0.10062012076377869, 0.15723145008087158, 0.15443088114261627, 0.13722744584083557, 0.305061012506485, 0.211642324924469, 0.10422084480524063, 0.2440488040447235, 0.09801960736513138, 0.24544909596443176, 0.12362472712993622, 0.2164432853460312, 0.12222444266080856, 0.2538507580757141, 0.16303260624408722, 0.09761952608823776, 0.2388477623462677, 0.1750349998474121, 0.21744349598884583, 0.19563913345336914, 0.17443488538265228, 0.14622925221920013, 0.17843568325042725, 0.09921984374523163, 0.15383076667785645, 0.1800360083580017, 0.10242048650979996, 0.21724344789981842, 0.19243848323822021, 0.1060212031006813, 0.1774354875087738, 0.16363272070884705, 0.17683537304401398, 0.2190438061952591, 0.1698339730501175, 0.16423285007476807, 0.37007400393486023, 0.36807361245155334, 0.30666133761405945, 0.18523705005645752, 0.26445290446281433, 0.2340468019247055, 0.19083817303180695, 0.09761952608823776, 0.11162232607603073, 0.42488497495651245, 0.201640322804451, 0.12002400308847427, 0.21324265003204346, 0.2090418040752411, 0.25705140829086304, 0.14822964370250702, 0.18803760409355164, 0.2580516040325165, 0.33126625418663025, 0.30446088314056396, 0.24244849383831024]
fedprox001 = [0.09881976246833801, 0.10242048650979996, 0.10502100735902786, 0.19863972067832947, 0.10262052714824677, 0.10302060097455978, 0.09761952608823776, 0.13482695817947388, 0.10962192714214325, 0.10202040523290634, 0.1380276083946228, 0.10362072288990021, 0.13002599775791168, 0.12022404372692108, 0.09921984374523163, 0.23544709384441376, 0.15203040838241577, 0.12222444266080856, 0.25525104999542236, 0.30626124143600464, 0.18523705005645752, 0.25505101680755615, 0.18043608963489532, 0.2836567461490631, 0.24264852702617645, 0.10062012076377869, 0.20264053344726562, 0.12602519989013672, 0.23084616661071777, 0.32126426696777344, 0.2490498125553131, 0.21144229173660278, 0.3156631290912628, 0.4380876123905182, 0.1746349334716797, 0.3224644958972931, 0.3518703877925873, 0.4234846830368042, 0.1230246052145958, 0.20144028961658478, 0.2586517333984375, 0.32886576652526855, 0.3786757290363312, 0.26245248317718506, 0.11522304266691208, 0.12662532925605774, 0.15463092923164368, 0.3594718873500824, 0.1622324436903, 0.1920384019613266, 0.2266453355550766, 0.27965593338012695, 0.3258651793003082, 0.3058611750602722, 0.18603721261024475, 0.2066413313150406, 0.4070814251899719, 0.4010802209377289, 0.45729145407676697, 0.35887178778648376]

print(sum(fedprox0001))
print(sum(fedprox001))
print(sum(fedprox1))

#plt.plot(fedprox,label="fedprox")
plt.plot(fedval,label="fedval")
plt.plot(fedavg,label="fedavg")
plt.plot(fedprox1,label="fedprox u=1")
plt.plot(fedprox0001,label="fedprox u=0.001")
plt.legend(loc='upper left')
plt.title(title)
plt.show()

################# END PLOTS!! ####################

# x = np.array([[2,1],[1,-3]])
# w = np.unravel_index(abs(x).argmax(), x.shape)
# print(x[w])

# m = create_model("cifar10")
# x=m.get_weights()

# print(np.sum([np.prod(list(v.shape)) for v in x]))
# x = np.array([[2,10],[2,-4]])
# y = np.array([[0,1],[1,1]])
# z = np.unravel_index(abs(x).argmax(), x.shape)

# s = "1234"
# for i in range(len(s)):
#     if i+1 < len(s):
#         print(s[i+1])

#print(np.sum([np.prod(list(v.shape)) for v in x]))

# y = model.get_weights()

# w = fl.common.ndarrays_to_parameters(x)
# print(type(w))
# z = fl.common.parameters_to_ndarrays(w)
# print(type(z))
# x = np.array([[1,1],[1,1]])
# x = x/2
# print(x)
# print(x[0])
# for i in range(len(x)):
#     z.append(np.subtract(x[i],y[i]))

#print(np.unravel_index(x[0].argmax(), x[0].shape))
# i = np.unravel_index(x[0].argmax(), x[0].shape)
# print(x[0][i])
# x[0][i] = 0
# print(np.unravel_index(x[0].argmax(), x[0].shape))

#x[0][i] = -10
#print(x[0][i])
#print(np.argmax(x[0]))


# if __name__ == '__main__':
#     m = TesterClass("emnist")
#     m.the_test()

# def fun(x,y,n):
#     z = x+y+n
#     return z

# class Test:
#     def __init__(self):
#         self.x = 5
#         self.y = 5
    
#     def func(self, w):
#         m = fun(self.x,self.y,w)
#         return m

#     def testet(self):
#         with Pool(5) as p:
#             print(p.map(self.func,[1,2,3]))

# if __name__ == '__main__':    
#     asd = Test()
#     asd.testet()

#(x_train, y_train), (_, _) = tf.keras.datasets.cifar10.load_data()
#ds = tfds.load('emnist', split='train', shuffle_files=True)
#train = pd.read_csv('../emnist/emnist-bymerge-train.csv', header=None)
#test = pd.read_csv('../archive/emnist-balanced-test.csv', header=None)

#x_train = train.iloc[:, 1:]
#y_train = train.iloc[:, 0]
#x_test = test.iloc[:, 1:]
#y_test = test.iloc[:, 0]

#x_train = x_train.values
#y_train = y_train.values
#x_test = x_test.values
#y_test = y_test.values
#del train, test

#print(x_train[0])

#def rotate(image):
#    image = image.reshape([28, 28])
#    image = np.fliplr(image)
#    image = np.rot90(image)
#    return image.reshape([28 * 28])
#x_train = np.apply_along_axis(rotate, 1, x_train)
#x_test = np.apply_along_axis(rotate, 1, x_test)

#x_train = x_train.reshape(len(x_train), 28, 28)
#x_test = x_test.reshape(len(x_test), 28, 28)
#x_train = x_train.astype('float32')
#x_test = x_test.astype('float32')
#y_test = np_utils.to_categorical(y_test, 47)
#y_train = np_utils.to_categorical(y_train, 47)

# print(tf.version.VERSION)

# x_test, y_test = get_test_val_ds("emnist")
# x_test = x_test[int(len(x_test)/2):int(len(x_test)-1)]
# y_test = y_test[int(len(y_test)/2):int(len(y_test)-1)]
# print(x_test[0])
# print(y_test[0])
#model = create_model("emnist")
#model.summary()
#model.fit(x_train, y_train, epochs=1, batch_size=128, validation_split=0.1)

#model.evaluate(x_test,y_test)

#x_train, y_train = get_train_ds(10, 1)
#x_test, y_test = get_test_val_ds()

#model = create_model_ascent()
#model.fit(x_test, y_test, epochs=1, batch_size=128, validation_split=0.1)
#model.evaluate(x_test,y_test)

#asd = model.get_weights()
#asd2 = model.get_weights()
#print(asd[1])
#r = []
#for i in range(len(asd)):
#    r.append(np.subtract(asd[i],asd2[i]))
#print(r[1])

#x = np.array([0.7336,0.7476,0.7280,0.7466,0.7243,0.7339,0.7435,0.7560, 0.7336, 0.7479])
#print(np.mean(x))
#print(np.std(x))
"""
#<class 'tensorflow.python.data.ops.dataset_ops.ShardDataset'>
#<class 'tuple'>
#<class 'tensorflow.python.framework.ops.EagerTensor'>

x = list(x_train.as_numpy_iterator())
features = []
labels = []
for i in range(len(x)):
    for j in range(len(x[i][0])):
        features.append(x[i][0][j])
        labels.append(x[i][1][j])
features = np.array(features)
labels = np.array(labels)
#x = tf.data.Dataset.from_tensor_slices(features,labels)
#x = x.batch(32)

random.seed(0)
for i in range(50):
    x = [0.0 for j in range(10)]
    x[random.randint(0,9)] = 1.0
    labels[random.randint(0,len(labels)-1)] = x

model = Sequential()

model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32,32,3)))
model.add(layers.BatchNormalization())
model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D(pool_size=(2,2)))
model.add(layers.Dropout(0.3))

model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D(pool_size=(2,2)))
model.add(layers.Dropout(0.5))

model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D(pool_size=(2,2)))
model.add(layers.Dropout(0.5))

model.add(layers.Flatten())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(10, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
"""
